{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcav import TCAV\n",
    "from train import train_model, eval_model\n",
    "from datasets import get_datasets_and_data_loaders, create_copy_of_train_set\n",
    "from utils import imshow, visualize_model, get_reduced_activation_space_points, scatter_plot_classes, get_sensitive_filenames\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da5fbd",
   "metadata": {},
   "source": [
    "### (1) Train a CNN classification model on top of the provided images and labels, achieving high predictive performance on the train set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8167f655",
   "metadata": {},
   "source": [
    "**Set training parameters and get the dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ac958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "LEARNING_RATE = 0.00001\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Set data directory\n",
    "data_dir = './data'\n",
    "\n",
    "# mean and std values the model's original training dataset\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Get data loaders for the data\n",
    "image_datasets, dataloaders, dataset_sizes, class_names, single_sample_train_dataloader = get_datasets_and_data_loaders(data_dir, mean, std, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b49ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset Classes: ', class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c879a1",
   "metadata": {},
   "source": [
    "**Display some of the images and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of the training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Display some images in the trainset\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, mean, std, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415af31",
   "metadata": {},
   "source": [
    "**Load a pre-trained model and define the loss criteria and optimizer; finetune on our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pretrained resnet152 model\n",
    "classification_model = torchvision.models.resnet152(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Replace the last fully connected layer with a layer that will predict scores for our two classes.\n",
    "num_ftrs = classification_model.fc.in_features\n",
    "classification_model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Move the model to the gpu\n",
    "classification_model = classification_model.to(device)\n",
    "\n",
    "# Define binary cross entropy as our loss criteria\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define an SGD optimizer for model parameters\n",
    "optimizer = optim.SGD(classification_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8aa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the model on our dataset\n",
    "classification_model = train_model(classification_model, criterion, optimizer, dataloaders, dataset_sizes, data_dir, num_epochs=NUM_EPOCHS, return_best_test_set_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7b25c",
   "metadata": {},
   "source": [
    "**Visualize some of the predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the model predictions on the train set\n",
    "visualize_model(classification_model, dataloaders, class_names, mean, std, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3472c12",
   "metadata": {},
   "source": [
    "**Print the named internal modules of our classification-model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6f135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the modules of the resnet152 network\n",
    "print('Classification Network Modules: ')\n",
    "print('------------------------------------------')\n",
    "print(classification_model.modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe4cda5",
   "metadata": {},
   "source": [
    "### (2) Visualise the model’s hidden activation space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3fc20",
   "metadata": {},
   "source": [
    "**Extract and visualize activation-space output from intermediate layers of the network on a portion of the train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb309da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 2D dimensionally reduced activation-space encoded vectors for named layers 1,2 and 3 of the resnet152 network for a portion of the train split\n",
    "layers_out = ['layer1','layer2','layer3']\n",
    "activations, activation_labels = get_reduced_activation_space_points(classification_model, layers_out, dataloaders['train'], num_samples = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99180ddb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a pca-dimensionally reduced scatter plot of train datapoints encoded to the level of layers 1, 2, and 3\n",
    "for layer in layers_out:\n",
    "    print(layer,'output')\n",
    "    print('-----------------------------')\n",
    "    scatter_plot_classes(activations[layer], activation_labels, [0, 1], {0:'r', 1:'g'}, ['classA', 'classB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fdf00",
   "metadata": {},
   "source": [
    "**Load Images of the “concept”** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load examples of the concept images\n",
    "with open(data_dir+'/concept_imgs.pkl','rb') as f:\n",
    "    concept_examples = pickle.load(f)\n",
    "\n",
    "print('Number of concept examples: ', len(concept_examples))\n",
    "\n",
    "# Display images\n",
    "out = torchvision.utils.make_grid(concept_examples)\n",
    "imshow(out, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a659e",
   "metadata": {},
   "source": [
    "**Set TCAV parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a1a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify tcav parameters:\n",
    "layer_out = 'layer2' # Use outputs from layer2 of the resnet model for our intermediate activation-space\n",
    "class_index = 0      # We plan to test the sensitivity the model for class 0 (enemies) detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e421b",
   "metadata": {},
   "source": [
    "# **NEGATIVE SAMPLES APPROACH:** \n",
    "Use negative examples that are uniformly randomly sampled from the train set (that may randomly include some Arnie samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tcav object using uniform random negative samples (that may include some concept samples)\n",
    "tcav = TCAV(classification_model, concept_examples, layer_out, class_index, random_dataset=image_datasets['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d253cd",
   "metadata": {},
   "source": [
    "### (4) Train TCAV to recognise the particular concept "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4f3bc",
   "metadata": {},
   "source": [
    "### (5) Visualise the CAV vector in the model’s hidden space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot a cav vector (classifying between random samples and our concept images in the intermediate activation-space)\n",
    "# Plotted at lower dimensionality\n",
    "cav = tcav.train_and_get_CAV(plot=True)\n",
    "\n",
    "print('Shape of the cav vector computed: ',cav.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c068eea2",
   "metadata": {},
   "source": [
    "### (6) Use the trained TCAV model to find images in the train set for which the concept is high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through every sample in the train set and find the filenames for images classified to be the particular concept by high tcav sensitivity (above the given threshold)\n",
    "concept_images_filenames = get_sensitive_filenames(tcav, single_sample_train_dataloader, threshold_sensitivity=0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
